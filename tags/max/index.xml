<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>max | Edgar Cutar Junior</title>
    <link>/tags/max/</link>
      <atom:link href="/tags/max/index.xml" rel="self" type="application/rss+xml" />
    <description>max</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 06 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/edgar.jpg</url>
      <title>max</title>
      <link>/tags/max/</link>
    </image>
    
    <item>
      <title>Ajuste de modelos preditivos com o pacote {parsnip}</title>
      <link>/post/parsnip/</link>
      <pubDate>Sun, 06 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/parsnip/</guid>
      <description>


&lt;p&gt;Neste &lt;a href=&#34;https://www.linkedin.com/pulse/pr%C3%A9-processamento-de-dados-r-com-o-pacote-recipes-edgar-cutar-junior/&#34;&gt;segundo post da série sobre o Tidymodels&lt;/a&gt;, o conjunto de pacotes desenvolvido por &lt;a href=&#34;twitter.com/topepos&#34;&gt;Max Kuhn&lt;/a&gt; para ajuste de modelos preditivos para R, iremos falar sobre o &lt;a href=&#34;https://tidymodels.github.io/parsnip/&#34;&gt;{parsnip}&lt;/a&gt;. Esse post saiu quase todo da &lt;a href=&#34;https://tidymodels.github.io/parsnip/&#34;&gt;própria página do pacote parsnip&lt;/a&gt; assim como de posts do Max Kuhn no RStudio (notadamente &lt;a href=&#34;https://www.tidyverse.org/blog/2019/04/parsnip-internals/&#34;&gt;este&lt;/a&gt; e &lt;a href=&#34;https://www.tidyverse.org/blog/2018/11/parsnip-0-0-1/&#34;&gt;este&lt;/a&gt;). Todas as eventuais piadas ruins são minhas mesmo.&lt;/p&gt;
&lt;div id=&#34;cenouras-e-pastinagas&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cenouras e Pastinagas&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;Parsnip_vs_carrot.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Parsnip é o nome em inglês da pastinaga, parente esquecida da cenoura. Originalmente parsnip era o pacote que iria substituir o &lt;a href=&#34;http://caret.r-forge.r-project.org/&#34;&gt;{caret}&lt;/a&gt; (a pronúncia de caret é igual a “carrot”, cenoura em inglês), que foi o primeiro pacote integrado para modelagem preditiva para R, desenvolvido no começo dos anos 2000 pelo mesmo Max Kuhn, na época diretor de pesquisa não-clínica na Pfizer.&lt;/p&gt;
&lt;p&gt;Com a evolução do pacote, a ideia originaldo Parsnip acabou se desdobrando no conjunto de pacotes que compõem o {tidymodels}, cada um deles especializado em tarefas específicas da modelagem preditiva. Com isso &lt;a href=&#34;https://www.tidyverse.org/blog/2018/11/parsnip-0-0-1/&#34;&gt;Parsnip ficou com um problema bem específico, a interface.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;uma-interface-padronizada&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Uma interface padronizada&lt;/h2&gt;
&lt;p&gt;Diversas funções e pacotes oferecem diferentes interfaces e parâmetros para objetivos parecidos, e o parsnip padroniza essa interface para o ajuste dos modelos e também para retornar os valores preditos.&lt;/p&gt;
&lt;div id=&#34;o-problema&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;O Problema&lt;/h3&gt;
&lt;p&gt;O problema de inconsistência de interface é facilmente encontrado em diversos modelos do R. Um exemplo simples acontece na regressão logística. Talvez o mais consagrado modo de &lt;a href=&#34;https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm&#34;&gt;ajustar uma regressão logística seja através do pacote &lt;code&gt;glm&lt;/code&gt;&lt;/a&gt;. Esse pacote usa a sintaxe padrão do R (&lt;a href=&#34;https://www.amazon.com.br/dp/B07737S8XJ/ref=dp-kindle-redirect?_encoding=UTF8&amp;amp;btkr=1&#34;&gt;que, na verdade, precede o R&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Para ajustar uma regressão logística no &lt;code&gt;glm&lt;/code&gt; você:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Usa o método da fórmula para definir as variáveis preditoras e o que deve ser predito.&lt;/li&gt;
&lt;li&gt;O método da fórmula usa o formato &lt;code&gt;Y ~ X1 + X2 + X3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Para especificar que é um modelo logístico usamos o argumento &lt;code&gt;family = binomial&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Agora vamos supor que eu queira aplicar alguma regularização nesse modelo. Uma escolha possível seria &lt;a href=&#34;https://cran.r-project.org/web/packages/glmnet/index.html&#34;&gt;usando o pacote &lt;code&gt;glmnet&lt;/code&gt;&lt;/a&gt;. Nesse caso:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Esse pacote não usa o método da fórmula, deve-se entregar os preditores em uma matriz (o que implica que variáveis “dummy” devem ser pré-computadas)&lt;/li&gt;
&lt;li&gt;O argumento da família muda ligeiramente, e deve ser &lt;code&gt;family = &#34;binomial&#34;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;O problema seria ainda maior se eu tentasse usar a interface para o TensorFlow no pacote &lt;code&gt;keras&lt;/code&gt;. O &lt;code&gt;keras&lt;/code&gt; tem uma abordagem muito interessante para encadear modelos de Deep Learning mas a formulação é completamente diferente do tradicional. Ajustar um modelo usando &lt;code&gt;keras&lt;/code&gt; exigiria estudar e entender uma outra forma de sintaxe de modelos.&lt;/p&gt;
&lt;p&gt;O problema se estende a como os diferentes pacotes retornam predições. A maior parte dos pacotes em R usam a função &lt;code&gt;predict()&lt;/code&gt;. Para retornar um vetor de probabilidades na nossa regressão logística, usaríamos &lt;code&gt;predict(obj, newdata, type = &#34;response&#34;)&lt;/code&gt;. Mas essa convenção varia bastante entre pacotes.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Função&lt;/th&gt;
&lt;th&gt;Pacote&lt;/th&gt;
&lt;th&gt;Código&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;glm&lt;/td&gt;
&lt;td&gt;stats&lt;/td&gt;
&lt;td&gt;predict(obj, type = “response”)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;lda&lt;/td&gt;
&lt;td&gt;MASS&lt;/td&gt;
&lt;td&gt;predict(obj)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;gbm&lt;/td&gt;
&lt;td&gt;gbm&lt;/td&gt;
&lt;td&gt;predict(obj, type = “response”, n.trees)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;mda&lt;/td&gt;
&lt;td&gt;mda&lt;/td&gt;
&lt;td&gt;predict(obj, type = “posterior”)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;rpart&lt;/td&gt;
&lt;td&gt;rpart&lt;/td&gt;
&lt;td&gt;predict(obj, type = “prob”)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Weka&lt;/td&gt;
&lt;td&gt;RWeka&lt;/td&gt;
&lt;td&gt;predict(obj, type = “probability”)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Numa instância maior, podemos ter ainda mais problemas: alguns modelos podem criar predições em vários submodelos de uma vez. Quando usamos Boosted Trees ajustadas com &lt;em&gt;i&lt;/em&gt; árvores, podemos fazer uma predição usando menos de &lt;em&gt;i&lt;/em&gt; iterações (efetivamente criando um novo modelo de predição), o que pode levar a mais inconsistências.&lt;/p&gt;
&lt;p&gt;Esse tipo de problema, quando agregado, podia levar ao questionamento&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;O R está trabalhando pra mim ou eu estou trabalhando para o R?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;a-solução&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A solução&lt;/h3&gt;
&lt;p&gt;Para demonstrar como o &lt;code&gt;parsnip&lt;/code&gt; funciona, vamos seguir com o exemplo da regressão logística.&lt;/p&gt;
&lt;p&gt;Para isso vamos usar uma base de dados chamada &lt;code&gt;Smarket&lt;/code&gt;, do pacote ISLR, que tem os retornos diários do índice S&amp;amp;P500 entre 2001 e 2005. Vamos começar dividindo a base em treino e teste. Vamos centralizar e escalar usando uma receita simples do pacote &lt;code&gt;recipes&lt;/code&gt;&lt;a href=&#34;(https://www.linkedin.com/pulse/pr%C3%A9-processamento-de-dados-r-com-o-pacote-recipes-edgar-cutar-junior/)&#34;&gt;(falamos sobre esse pacote no último post)&lt;/a&gt;].&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if (!require(&amp;quot;pacman&amp;quot;)) install.packages(&amp;quot;pacman&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Carregando pacotes exigidos: pacman&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pacman::p_load(tidymodels, ISLR) 

split &amp;lt;- initial_split(Smarket %&amp;gt;% select(-Year, -Today), props = 9/10)
smarket_train &amp;lt;- training(split)
smarket_test  &amp;lt;- testing(split)

smarket_rec &amp;lt;- recipe(Direction ~ ., data = smarket_train) %&amp;gt;%
  step_center(all_predictors()) %&amp;gt;%
  step_scale(all_predictors()) %&amp;gt;%
  prep(training = smarket_train, retain = TRUE)

train_data &amp;lt;- juice(smarket_rec)
test_data  &amp;lt;- bake(smarket_rec, smarket_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Para usar o &lt;code&gt;parsnip&lt;/code&gt;, você começa com a especificação de um modelo. É um objeto simples que define a intenção daquele modelo. Já que vamos seguir com nossa saga de regressão logística, nosso primeiro passo é uma função simples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;market_model &amp;lt;- logistic_reg()
market_model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Logistic Regression Model Specification (classification)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pode parecer estranho porque não entramos com absolutamente nenhum detalhe sobre o que vamos fazer, mas é isso aí mesmo! O &lt;code&gt;parsnip&lt;/code&gt; oferece uma variedade de formas para ajustar esse modelo. Nós vamos usar o tradicional Método dos Mínimos Quadrados, mas poderia ser com penalização (via lasso, ridge, Bayes etc)… Diferenciamos um caso do outro através da &lt;em&gt;&lt;em&gt;Engine&lt;/em&gt; computacional&lt;/em&gt;, que é uma combinação de tipo de estimação com implementação. Pode ser através de um pacote ou uma plataforma como Spark ou TensorFlow. Para começar simples, vamos usar o glm.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm_market_model &amp;lt;- market_model %&amp;gt;% set_engine(&amp;quot;glm&amp;quot;)
glm_market_model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Logistic Regression Model Specification (classification)
## 
## Computational engine: glm&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Não existem mais muitos argumentos por aqui, então vamos pular direto pro ajuste do modelo. Nossas duas escolhas nesse ponto são entre usar &lt;code&gt;fit()&lt;/code&gt; ou &lt;code&gt;fit_xy()&lt;/code&gt;. O primeiro usa o método da fórmula, enquanto o segundo usa objetos separados para os preditores e para o resultado.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm_fit &amp;lt;- glm_market_model %&amp;gt;%
  fit(Direction ~ ., data = train_data)

#ou

glm_market_model %&amp;gt;%
  fit_xy(x = select(train_data, -Direction), y = select(train_data, Direction))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## parsnip model object
## 
## Fit time:  20ms 
## 
## Call:  stats::glm(formula = formula, family = stats::binomial, data = data)
## 
## Coefficients:
## (Intercept)         Lag1         Lag2         Lag3         Lag4         Lag5  
##    0.103028    -0.133905    -0.043881     0.011143     0.002595     0.008425  
##      Volume  
##    0.047614  
## 
## Degrees of Freedom: 937 Total (i.e. Null);  931 Residual
## Null Deviance:       1298 
## Residual Deviance: 1293  AIC: 1307&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Claro que não é necessário fazer todos esses passos individualmente, e poderíamos simplesmente condensar tudo em um só objeto.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glm_fit &amp;lt;- logistic_reg() %&amp;gt;% 
  set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;% 
  fit(Direction ~ ., data = train_data)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mais-engines&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mais engines!&lt;/h3&gt;
&lt;p&gt;O valor do &lt;code&gt;parsnip&lt;/code&gt; começa quando queremos testar diferentes &lt;em&gt;engines&lt;/em&gt;. Vamos usar o mesmo modelo e estimar os coeficientes através de estimativa Bayesiana usando &lt;code&gt;stan&lt;/code&gt;. Para isso só precisamos:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stan_model &amp;lt;- logistic_reg() %&amp;gt;% 
  set_engine(&amp;quot;stan&amp;quot;)

stan_model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Logistic Regression Model Specification (classification)
## 
## Computational engine: stan&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Para ajustar esse modelo, o &lt;code&gt;parsnip&lt;/code&gt; chamou a função &lt;code&gt;stan_glm()&lt;/code&gt; do pacote &lt;code&gt;rstanarm&lt;/code&gt;. Se você quiser passar argumentos para essa função, simplesmente adicione eles na função &lt;code&gt;set_engine()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stan_model &amp;lt;- logistic_reg() %&amp;gt;% 
  set_engine(&amp;quot;stan&amp;quot;, iter = 5000) 

stan_model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Logistic Regression Model Specification (classification)
## 
## Engine-Specific Arguments:
##   iter = 5000
## 
## Computational engine: stan&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;O modelo pode ser ajustado da mesma forma. &lt;code&gt;rstanarm&lt;/code&gt; printa &lt;em&gt;MUITAS&lt;/em&gt; informações ao ajustar um modelo. Isso pode ser útil para diagnóstico mas vamos excluir usando uma função de controle.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ctrl &amp;lt;- fit_control(verbosity = 0)

stan_fit &amp;lt;- stan_model %&amp;gt;%
    fit(Direction ~ ., data = train_data, control = ctrl)

stan_fit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## parsnip model object
## 
## Fit time:  7.9s 
## stan_glm
##  family:       binomial [logit]
##  formula:      Direction ~ .
##  observations: 938
##  predictors:   7
## ------
##             Median MAD_SD
## (Intercept)  0.1    0.1  
## Lag1        -0.1    0.1  
## Lag2         0.0    0.1  
## Lag3         0.0    0.1  
## Lag4         0.0    0.1  
## Lag5         0.0    0.1  
## Volume       0.0    0.1  
## 
## ------
## * For help interpreting the printed output see ?print.stanreg
## * For info on the priors used see ?prior_summary.stanreg&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;predições&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Predições&lt;/h3&gt;
&lt;p&gt;Além das óbvias funcionalidades para ajustar diferentes modelos, ainda temos muitas vantagens na hora de prever resultados. Isso resolve uma série de frustrações como ter um arquivo predito que PULA algum resultado quando tem valores faltando, por exemplo, além de padronizar a função.&lt;/p&gt;
&lt;p&gt;Para regressões logísticas, por exemplo, o output é sempre um &lt;code&gt;tibble&lt;/code&gt; com uma coluna &lt;code&gt;.pred_class&lt;/code&gt; contendo a classe predita.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(glm_fit, test_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 312 x 1
##    .pred_class
##    &amp;lt;fct&amp;gt;      
##  1 Down       
##  2 Up         
##  3 Up         
##  4 Up         
##  5 Down       
##  6 Up         
##  7 Up         
##  8 Up         
##  9 Down       
## 10 Up         
## # ... with 302 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(stan_fit, test_data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 312 x 1
##    .pred_class
##    &amp;lt;fct&amp;gt;      
##  1 Down       
##  2 Up         
##  3 Up         
##  4 Up         
##  5 Down       
##  6 Up         
##  7 Up         
##  8 Up         
##  9 Down       
## 10 Up         
## # ... with 302 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Isso facilita a união com os valores originais e o &lt;code&gt;.&lt;/code&gt; no nome é para evitar nomes duplicados.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;parsnip&lt;/code&gt; também trás diferentes tipos de previsão com uma interface padrão. Por exemplo, para estimativa de intervalos.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(glm_fit, test_data, type = &amp;quot;conf_int&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 312 x 4
##    .pred_lower_Down .pred_upper_Down .pred_lower_Up .pred_upper_Up
##               &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt;
##  1            0.429            0.611          0.389          0.571
##  2            0.439            0.553          0.447          0.561
##  3            0.400            0.557          0.443          0.600
##  4            0.396            0.544          0.456          0.604
##  5            0.436            0.586          0.414          0.564
##  6            0.329            0.509          0.491          0.671
##  7            0.396            0.549          0.451          0.604
##  8            0.313            0.648          0.352          0.687
##  9            0.449            0.676          0.324          0.551
## 10            0.251            0.500          0.500          0.749
## # ... with 302 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ou para as probabilidades de cada previsão:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict(glm_fit, test_data, type = &amp;quot;prob&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 312 x 2
##    .pred_Down .pred_Up
##         &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1      0.521    0.479
##  2      0.496    0.504
##  3      0.478    0.522
##  4      0.469    0.531
##  5      0.511    0.489
##  6      0.416    0.584
##  7      0.472    0.528
##  8      0.478    0.522
##  9      0.566    0.434
## 10      0.367    0.633
## # ... with 302 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;próximos-passos&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Próximos passos&lt;/h2&gt;
&lt;p&gt;Esse artigo apenas toca a superfície das possibilidades de uso do pacote &lt;code&gt;parsnip&lt;/code&gt;. Nos próximos artigos vamos explorar como fazer uma &lt;em&gt;grid search&lt;/em&gt; pelos melhores parâmetros, como fazer uma &lt;em&gt;k-fold Cross Validation&lt;/em&gt; usando esta interface e como avaliar a performance de modelos, tudo isso usando os tidymodels.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Pré-processamento de dados com o pacote {recipes}</title>
      <link>/post/recipes/</link>
      <pubDate>Sun, 06 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/recipes/</guid>
      <description>


&lt;p&gt;Técnicas de pré-processamento de dados, também chamadas de &lt;em&gt;Feature Engineering&lt;/em&gt; são aquelas em que se adiciona, remove ou transforma a base de dados, tendo em vista a modelagem preditiva. Embora esse texto esteja primariamente preocupado com o &lt;strong&gt;como fazer&lt;/strong&gt;, é muito importante entender o porquê de se fazer cada uma destas transformações. Recomendo bastante os livros do &lt;a href=&#34;http://twitter.com/topepos&#34;&gt;Max Kuhn&lt;/a&gt;, Software Engineer do RStudio e autor do pacote &lt;code&gt;{recipes}&lt;/code&gt; de que vamos tratar hoje. Disponível online gratuitamente está o &lt;a href=&#34;http://www.feat.engineering/index.html&#34;&gt;&lt;em&gt;Feature Engineering and Selection&lt;/em&gt;&lt;/a&gt; e na ala paga está o maravilhoso &lt;a href=&#34;appliedpredictivemodeling.com/&#34;&gt;&lt;em&gt;Applied Predictive Modeling&lt;/em&gt;&lt;/a&gt;, além da &lt;a href=&#34;https://tidymodels.github.io/recipes&#34;&gt;página do pacote &lt;code&gt;{recipes}&lt;/code&gt;&lt;/a&gt; de onde muito desse post saiu.&lt;/p&gt;
&lt;p&gt;Cada tipo de modelo exige os dados mastigados de uma certa maneira. Modelos baseados em árvores (como &lt;a href=&#34;https://en.wikipedia.org/wiki/Random_forest&#34;&gt;Random Forests&lt;/a&gt; e &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting&#34;&gt;Boosted Trees&lt;/a&gt;) geralmente exigem pouco pré-processamento e podem lidar até com variáveis categóricas diretamente, enquanto outros algoritmos como &lt;a href=&#34;https://en.wikipedia.org/wiki/Support-vector_machine&#34;&gt;Support-Vector Machines&lt;/a&gt; e &lt;a href=&#34;https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline&#34;&gt;Multivariate Adaptive Regression Splines (MARS)&lt;/a&gt; são mais exigentes com o formato com que recebem as variáveis preditoras para que funcionem corretamente. Um pré-processamento bem feito pode ser a diferença entre um modelo preditivo bem ou mal sucedido.&lt;/p&gt;
&lt;div id=&#34;o-pacote-recipes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;O pacote &lt;code&gt;{recipes}&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;No começo dos anos 2000, Max Kuhn lançou o pacote &lt;code&gt;{caret}&lt;/code&gt; (caret é um anagrama para &lt;em&gt;Classification And REgression Training&lt;/em&gt;) no &lt;a href=&#34;https://cran.r-project.org&#34;&gt;CRAN&lt;/a&gt;. Esse pacote se propõe a fazer tudo dentro da modelagem preditiva, desde o pré processamento, treinamento, validação, validação cruzada e etc etc etc. &lt;a href=&#34;https://player.fm/series/series-2098032/tidymodels-with-max-kuhn&#34;&gt;O próprio Max já admitiu que dar manutenção neste pacote é um pesadelo&lt;/a&gt;. A preocupação com a retrocompatibilidade e a quantidade de algoritmos presentes no &lt;code&gt;{caret}&lt;/code&gt; tem impedido atualizações massivas no pacote. Desde que entrou para a equipe do Rstudio, no final de 2016, o objetivo de ambos (Max e RStudio) tem sido o de desenvolver uma interface integrada para modelos preditivos no R, condizente com a &lt;a href=&#34;https://cran.r-project.org/web/packages/tidyverse/vignettes/manifesto.html&#34;&gt;filosofia e o manifesto que norteiam o &lt;code&gt;tidyverse&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Ao invés de “refazer” o &lt;code&gt;{caret}&lt;/code&gt;, a estratégia adotada foi o de construir uma nova &lt;em&gt;suit&lt;/em&gt; de pacotes integrados voltados para modelagem preditiva, da mesma forma que o &lt;code&gt;tidyverse&lt;/code&gt; é voltado para a &lt;strong&gt;Ciência de Dados&lt;/strong&gt; num &lt;a href=&#34;https://r4ds.had.co.nz/introduction.html&#34;&gt;sentido mais amplo&lt;/a&gt;, conforme mostrado pelo &lt;strong&gt;Diagrama de Hadley&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;diagram.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Essa &lt;em&gt;suit&lt;/em&gt; de pacotes integrados para modelagem preditiva se chama &lt;code&gt;tidymodels&lt;/code&gt;. Dentro do &lt;code&gt;tidymodels&lt;/code&gt; existem já &lt;a href=&#34;https://www.tidyverse.org/articles/2018/08/tidymodels-0-0-1/&#34;&gt;vários pacotes&lt;/a&gt;, entre eles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;rsamples&lt;/code&gt;: voltado para criar e resumir diferentes tipos de amostragens.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;recipes&lt;/code&gt;: uma &lt;em&gt;engine&lt;/em&gt; de pré-processamento para gerar uma &lt;em&gt;design matrices&lt;/em&gt; para modelagem ou visualização.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;parsnip&lt;/code&gt;: uma interface unificada para treinamento de modelos na filosofia &lt;em&gt;tidy&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;yardstick&lt;/code&gt;: métodos para mensurar a performance de modelos;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;corrr&lt;/code&gt;: um pacote para explorar correlações.&lt;/li&gt;
&lt;li&gt;entre diversos outros ainda em desenvolvimento. Dê uma olhada &lt;a href=&#34;https://github.com/tidymodels&#34;&gt;aqui&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;criando-uma-receita&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Criando uma receita&lt;/h2&gt;
&lt;p&gt;O &lt;code&gt;{recipes}&lt;/code&gt; têm como objetivo principal entregar uma &lt;a href=&#34;https://en.wikipedia.org/wiki/Design_matrix&#34;&gt;matriz de design (&lt;em&gt;design matrix&lt;/em&gt;)&lt;/a&gt; pronta para ser utilizada no ajuste de modelos preditivos. Direto da &lt;em&gt;Wikipedia&lt;/em&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Na Estatística, uma &lt;strong&gt;matriz de design&lt;/strong&gt;, também conhecida como &lt;strong&gt;matriz de modelo&lt;/strong&gt; ou &lt;strong&gt;matriz regressora&lt;/strong&gt; e também chamada de &lt;strong&gt;X&lt;/strong&gt;, é uma matriz de valores de variáveis explanatórias de um conjunto de objetos.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;O R possui métodos já estabelecidos para gerar esses tipo de matrizes (como as &lt;a href=&#34;https://www.rstudio.com/rviews/2017/02/01/the-r-formula-method-the-good-parts&#34;&gt;fórmulas&lt;/a&gt; e a função &lt;code&gt;model.matrix()&lt;/code&gt;), mas existem &lt;a href=&#34;https://rviews.rstudio.com/2017/03/01/the-r-formula-method-the-bad-parts/&#34;&gt;algumas limitações sobre o que eles podem fazer&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A ideia do &lt;code&gt;recipes&lt;/code&gt;, portanto é definir uma &lt;strong&gt;receita&lt;/strong&gt; de pré-processamento que possa ser usada sequencialmente para definir a codificação e o pré-processamento dos dados.&lt;/p&gt;
&lt;div id=&#34;criando-uma-receita-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Criando uma receita&lt;/h3&gt;
&lt;p&gt;A receita é criada a partir da função &lt;code&gt;recipe()&lt;/code&gt;. Um exemplo abaixo usando o dataset &lt;code&gt;Sonar&lt;/code&gt; do pacote &lt;code&gt;mlbench&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if (!require(&amp;quot;pacman&amp;quot;)) install.packages(&amp;quot;pacman&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Carregando pacotes exigidos: pacman&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pacman::p_load(recipes, mlbench) 

data(Sonar)

Sonar %&amp;gt;% 
  recipe(formula = Class ~ .)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         60&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ao ativar a função, é necessário indicar quais as variáveis de entrada e saída do modelo.
Existem algumas formas de fazer isso. Uma delas é o método da fórmula, usado acima.&lt;/p&gt;
&lt;p&gt;Esse modelo assume o formato &lt;span class=&#34;math inline&#34;&gt;\(Y \sim X_1 + X_2 + ... + X_n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Acima, o &lt;code&gt;.&lt;/code&gt; indica que todas as demais variáveis devem estar do lado direito da fórmula (é mais fácil que escrever as 60 variáveis uma a uma).&lt;/p&gt;
&lt;p&gt;Existem &lt;a href=&#34;https://tidymodels.github.io/recipes/articles/Selecting_Variables.html&#34;&gt;outras formas de selecionar variáveis dentro do pacote &lt;code&gt;{recipes}&lt;/code&gt;&lt;/a&gt;, como a função &lt;code&gt;update_role()&lt;/code&gt;. Essa função é útil especialmente quando sua tabela não contém apenas variáveis de preditoras e de saídas (&lt;code&gt;predictors&lt;/code&gt; e &lt;code&gt;outcome&lt;/code&gt;). Abaixo um exemplo usando esta função:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(biomass)

biomass %&amp;gt;% 
  recipe() %&amp;gt;% 
  update_role(HHV, 
              new_role = &amp;quot;outcome&amp;quot;) %&amp;gt;% 
  update_role(carbon, hydrogen, oxygen, nitrogen, sulfur,
           new_role = &amp;quot;predictor&amp;quot;) %&amp;gt;% 
  update_role(sample, 
              new_role = &amp;quot;id variable&amp;quot;) %&amp;gt;% 
  update_role(dataset, 
              new_role = &amp;quot;splitting indicator&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data Recipe
## 
## Inputs:
## 
##                 role #variables
##          id variable          1
##              outcome          1
##            predictor          5
##  splitting indicator          1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;acessando-variaveis-por-seu-papel&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Acessando variáveis por seu papel&lt;/h3&gt;
&lt;p&gt;Estes “papéis” (roles), atribuídos nessa etapa, vão ser usados nas fases subsequentes. É possível indicar os seguintes papéis através de atalhos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;has_role()&lt;/code&gt;: acessa todas as variáveis com papéis determinados&lt;/li&gt;
&lt;li&gt;&lt;code&gt;all_outcome()&lt;/code&gt;: acessa a variável determinada como Y do modelo&lt;/li&gt;
&lt;li&gt;&lt;code&gt;all_predictors()&lt;/code&gt;: acessa todas as variáveis determinadas como preditoras&lt;/li&gt;
&lt;li&gt;&lt;code&gt;all_numeric()&lt;/code&gt;: acessa todas as variáveis numéricas&lt;/li&gt;
&lt;li&gt;&lt;code&gt;all_nominal&lt;/code&gt;: acessa todas as variáveis categóricas.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;steps&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Steps&lt;/h3&gt;
&lt;p&gt;A partir da criação da receita e a determinação dos papéis, é hora do pré-processamento real. Cada &lt;strong&gt;passo&lt;/strong&gt; do pré-processamento é adicionado através de, pasme, &lt;strong&gt;passos&lt;/strong&gt; (em inglês, &lt;em&gt;steps&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Os passos disponíveis, a grosso modo, podem ser divididos nos seguintes grupos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Imputação&lt;/li&gt;
&lt;li&gt;Transformações individuais para assimetria e outras questões&lt;/li&gt;
&lt;li&gt;Discretização (caso necessário e caso você não tenha outra opção)&lt;/li&gt;
&lt;li&gt;Criação de variáveis &lt;em&gt;dummy&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Criação de interações&lt;/li&gt;
&lt;li&gt;Passos de normalização (centralizar, escalar, etc)&lt;/li&gt;
&lt;li&gt;Transformações multivariadas (PCA, &lt;em&gt;spatial-sign&lt;/em&gt;, etc)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Abaixo, usando o primeiro exemplo, vamos criar centralizar e escalar nossa receita:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(Sonar)

Sonar %&amp;gt;% 
  recipe(formula = Class ~ .) %&amp;gt;% 
  step_center(all_predictors()) %&amp;gt;% 
  step_scale(all_predictors())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         60
## 
## Operations:
## 
## Centering for all_predictors
## Scaling for all_predictors&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Na atual versão (0.1.7), existem 74 passos possíveis, com mais sendo desenvolvidos e implementados a todo momento. Vale lembrar que o pacote recipes ainda se encontra em fase de “maturação”, ou seja, muita coisa bacana ainda vem por aí. Abaixo a lista dos passos disponíveis hoje.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ls(&amp;quot;package:recipes&amp;quot;, pattern = &amp;quot;^step_&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;step_arrange&amp;quot;       &amp;quot;step_bagimpute&amp;quot;     &amp;quot;step_bin2factor&amp;quot;   
##  [4] &amp;quot;step_BoxCox&amp;quot;        &amp;quot;step_bs&amp;quot;            &amp;quot;step_center&amp;quot;       
##  [7] &amp;quot;step_classdist&amp;quot;     &amp;quot;step_corr&amp;quot;          &amp;quot;step_count&amp;quot;        
## [10] &amp;quot;step_date&amp;quot;          &amp;quot;step_depth&amp;quot;         &amp;quot;step_discretize&amp;quot;   
## [13] &amp;quot;step_downsample&amp;quot;    &amp;quot;step_dummy&amp;quot;         &amp;quot;step_factor2string&amp;quot;
## [16] &amp;quot;step_filter&amp;quot;        &amp;quot;step_geodist&amp;quot;       &amp;quot;step_holiday&amp;quot;      
## [19] &amp;quot;step_hyperbolic&amp;quot;    &amp;quot;step_ica&amp;quot;           &amp;quot;step_integer&amp;quot;      
## [22] &amp;quot;step_interact&amp;quot;      &amp;quot;step_intercept&amp;quot;     &amp;quot;step_inverse&amp;quot;      
## [25] &amp;quot;step_invlogit&amp;quot;      &amp;quot;step_isomap&amp;quot;        &amp;quot;step_knnimpute&amp;quot;    
## [28] &amp;quot;step_kpca&amp;quot;          &amp;quot;step_kpca_poly&amp;quot;     &amp;quot;step_kpca_rbf&amp;quot;     
## [31] &amp;quot;step_lag&amp;quot;           &amp;quot;step_lincomb&amp;quot;       &amp;quot;step_log&amp;quot;          
## [34] &amp;quot;step_logit&amp;quot;         &amp;quot;step_lowerimpute&amp;quot;   &amp;quot;step_meanimpute&amp;quot;   
## [37] &amp;quot;step_medianimpute&amp;quot;  &amp;quot;step_modeimpute&amp;quot;    &amp;quot;step_mutate&amp;quot;       
## [40] &amp;quot;step_mutate_at&amp;quot;     &amp;quot;step_naomit&amp;quot;        &amp;quot;step_nnmf&amp;quot;         
## [43] &amp;quot;step_normalize&amp;quot;     &amp;quot;step_novel&amp;quot;         &amp;quot;step_ns&amp;quot;           
## [46] &amp;quot;step_num2factor&amp;quot;    &amp;quot;step_nzv&amp;quot;           &amp;quot;step_ordinalscore&amp;quot; 
## [49] &amp;quot;step_other&amp;quot;         &amp;quot;step_pca&amp;quot;           &amp;quot;step_pls&amp;quot;          
## [52] &amp;quot;step_poly&amp;quot;          &amp;quot;step_profile&amp;quot;       &amp;quot;step_range&amp;quot;        
## [55] &amp;quot;step_ratio&amp;quot;         &amp;quot;step_regex&amp;quot;         &amp;quot;step_relu&amp;quot;         
## [58] &amp;quot;step_rename&amp;quot;        &amp;quot;step_rename_at&amp;quot;     &amp;quot;step_rm&amp;quot;           
## [61] &amp;quot;step_rollimpute&amp;quot;    &amp;quot;step_sample&amp;quot;        &amp;quot;step_scale&amp;quot;        
## [64] &amp;quot;step_shuffle&amp;quot;       &amp;quot;step_slice&amp;quot;         &amp;quot;step_spatialsign&amp;quot;  
## [67] &amp;quot;step_sqrt&amp;quot;          &amp;quot;step_string2factor&amp;quot; &amp;quot;step_unknown&amp;quot;      
## [70] &amp;quot;step_unorder&amp;quot;       &amp;quot;step_upsample&amp;quot;      &amp;quot;step_window&amp;quot;       
## [73] &amp;quot;step_YeoJohnson&amp;quot;    &amp;quot;step_zv&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;finalizando-a-receita-e-cozinhando-novos-dados&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Finalizando a receita e “cozinhando” novos dados&lt;/h3&gt;
&lt;p&gt;Todos os passos, até agora, simplesmente foram enumerados, nenhum deles é efetivamente calculado até que você entre com a função &lt;code&gt;prep()&lt;/code&gt;, que literalmente prepara sua receita.&lt;/p&gt;
&lt;p&gt;Após, é possível gerar os dados pré-processados a partir de duas funções:
- função &lt;code&gt;juice()&lt;/code&gt;: essa função “espreme” os dados usados para a criação da receita
- função &lt;code&gt;bake()&lt;/code&gt;: aplica a mesma série de passos em uma nova base de dados.&lt;/p&gt;
&lt;p&gt;Abaixo vamos separar a base de dados &lt;code&gt;credit_data&lt;/code&gt; em uma base de treino e uma de teste para ver o funcionamento completo do pacote.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pacman::p_load(rsample)

data(&amp;quot;credit_data&amp;quot;)

set.seed(55)
train_test_split &amp;lt;- initial_split(credit_data, prop = 0.75)

credit_train &amp;lt;- training(train_test_split)
credit_test &amp;lt;- testing(train_test_split)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Criadas as bases de treino e teste, vamos criar uma receita simples. Perceba o uso do parâmetro &lt;code&gt;retain = TRUE&lt;/code&gt;. Com isso, forçamos que a receita guarde a base de treino original, só assim podemos usar a função &lt;code&gt;juice()&lt;/code&gt;. Em bases muito grandes, precisamos ter cuidado com o tamanho da receita, caso esse parâmetro seja acionado.&lt;/p&gt;
&lt;p&gt;Vamos usar &lt;code&gt;step_knnimpute()&lt;/code&gt; para preencher os &lt;code&gt;NAs&lt;/code&gt; com seus vizinhos mais próximos e &lt;code&gt;step_dummy()&lt;/code&gt; para transformar nossas variáveis categóricas em variáveis dummy.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rec &amp;lt;- credit_train %&amp;gt;%
  recipe(Status ~ .) %&amp;gt;%
  step_knnimpute(all_predictors()) %&amp;gt;% 
  step_dummy(all_predictors(), -all_numeric()) %&amp;gt;% 
  step_center(all_predictors()) %&amp;gt;% 
  step_scale(all_predictors()) %&amp;gt;%
  prep(retain = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Agora temos uma receita pronta para ser aplicada. A receita guarda o modelo k-nearest-neighbour criado, as médias e desvios padrões, assim como qualquer outro dado necessário para pré-processar sua base.&lt;/p&gt;
&lt;p&gt;Primeiro vamos retirar a base de treino pré-processada, para isso, basta usar a função &lt;code&gt;juice()&lt;/code&gt; na receita. A seguir, vamos pré-processar a base de treino usando a função &lt;code&gt;bake()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_data &amp;lt;- rec %&amp;gt;% juice()
test_data &amp;lt;- rec %&amp;gt;% bake(credit_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Temos agora uma base pré-processada e pronta para uso, seja em modelagem, seja para visualização.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3,341 x 23
##    Seniority   Time      Age Expenses Income Assets    Debt  Amount   Price
##        &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1     0.133  0.921 -0.635      0.885 -0.171 -0.449 -0.273  -0.488  -0.996 
##  2     1.12   0.921  1.92      -0.386 -0.146 -0.449 -0.273  -0.0646  0.343 
##  3    -0.979  0.921 -1.18       0.377  0.492 -0.240 -0.273  -0.276  -0.206 
##  4    -0.979 -0.706 -1.00      -0.488 -0.446 -0.449 -0.273  -1.53   -0.890 
##  5    -0.856  0.921 -0.0873     0.987  0.893 -0.157 -0.273  -0.806   0.322 
##  6    -0.979  0.921 -0.453      1.75  -0.446  0.804 -0.273   0.359   0.836 
##  7    -0.979  0.108  0.369      1.75  -0.784 -0.449 -0.273   0.359   0.0299
##  8    -0.238  0.108 -0.270      0.224 -0.221 -0.115 -0.273   0.253   0.210 
##  9    -0.114 -0.706 -0.727      0.224 -0.271 -0.199 -0.273  -0.806  -0.882 
## 10     1.37  -0.706  0.00402    0.987  0.342 -0.157 -0.0520 -0.912  -0.841 
## # ... with 3,331 more rows, and 14 more variables: Status &amp;lt;fct&amp;gt;,
## #   Home_other &amp;lt;dbl&amp;gt;, Home_owner &amp;lt;dbl&amp;gt;, Home_parents &amp;lt;dbl&amp;gt;,
## #   Home_priv &amp;lt;dbl&amp;gt;, Home_rent &amp;lt;dbl&amp;gt;, Marital_married &amp;lt;dbl&amp;gt;,
## #   Marital_separated &amp;lt;dbl&amp;gt;, Marital_single &amp;lt;dbl&amp;gt;, Marital_widow &amp;lt;dbl&amp;gt;,
## #   Records_yes &amp;lt;dbl&amp;gt;, Job_freelance &amp;lt;dbl&amp;gt;, Job_others &amp;lt;dbl&amp;gt;,
## #   Job_partime &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Basicamente é isso aí. Nos próximos posts, vou tentar mostrar um pouco mais dos demais pacotes do &lt;code&gt;tidymodels&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Achou algum erro? &lt;a href=&#34;http://twitter.com/edgar_cutar&#34;&gt;Me avise no twitter&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
